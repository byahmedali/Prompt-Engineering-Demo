{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f333a4d",
   "metadata": {},
   "source": [
    "# **Prompt Engineering Demo**\n",
    "\n",
    "This notebook demonstrates prompt engineering principles. It contains explanations, code examples (zero-shot and few-shot), tips on *task, context, examples, role, format, tone*, and guidance for using chain-of-thought (CoT) techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fd16db",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Make sure you set your `GROQ_API_KEY` in the environment before running any cells that call the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c60579e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import os\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b6e6e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GROQ API key from .env file\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5adf4671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a helper function around Groq's Chat Completion\n",
    "client = Groq(api_key=api_key)\n",
    "\n",
    "def get_completion(prompt, model=\"llama3-8b-8192\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        model=model,\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc804694",
   "metadata": {},
   "source": [
    "# Components of a Prompt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93a33f4",
   "metadata": {},
   "source": [
    "## 1. Task\n",
    "\n",
    "The explicit instruction stating what you want the model to do.\n",
    "\n",
    "Keep it clear, specific. and focused on a single taks at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4972c2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are three brief bullet points explaining the importance of low latency Large Language Models (LLMs):\n",
      "\n",
      "• **Real-time interactions**: Low latency LLMs enable real-time interactions, such as conversational AI, chatbots, and voice assistants, which require fast response times to maintain user engagement and satisfaction. High latency can lead to frustrating delays, causing users to abandon interactions.\n",
      "\n",
      "• **Time-critical applications**: Low latency LLMs are crucial for applications that require rapid processing, such as natural language processing (NLP) for autonomous vehicles, medical diagnosis, or financial trading. In these scenarios, even a few milliseconds of latency can have significant consequences.\n",
      "\n",
      "• **Scalability and efficiency**: Low latency LLMs can be deployed in cloud-based services, allowing for efficient scaling and cost savings. By reducing latency, developers can build more responsive and scalable applications, which is essential for meeting the demands of modern users who expect fast and seamless interactions.\n"
     ]
    }
   ],
   "source": [
    "# Task-only (Zero-shot)\n",
    "prompt = \"Explain the importance of low latency LLMs in 3 brief bullet points.\"\n",
    "\n",
    "print(get_completion(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e07b3e2",
   "metadata": {},
   "source": [
    "## 2. Context\n",
    "\n",
    "Any background information or constraints that help the model produce a more accurate result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9c046b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello students! Today, we're going to talk about the importance of low latency Large Language Models (LLMs). You might be wondering, what's the big deal about latency? Well, let me explain.\n",
      "\n",
      "Latency refers to the time it takes for a system to respond to a request or input. In the context of LLMs, latency is critical because it directly affects the user experience. Imagine you're trying to have a conversation with a friend, but there's a 5-second delay between your responses. It would be frustrating, right? That's what happens when you're dealing with high latency LLMs.\n",
      "\n",
      "Low latency LLMs are essential because they enable fast and seamless interactions between humans and machines. This is particularly important in applications where speed and responsiveness are crucial. Here are three real-world examples where latency matters:\n",
      "\n",
      "1. **Virtual Assistants**: Virtual assistants like Siri, Google Assistant, or Alexa rely heavily on LLMs to understand and respond to voice commands. Low latency is crucial to ensure that the assistant can quickly respond to your queries, making the interaction feel natural and intuitive.\n",
      "2. **Chatbots**: Chatbots are used in various industries, such as customer service, healthcare, or finance. When a user interacts with a chatbot, they expect a quick response to their questions or concerns. High latency can lead to frustration and abandonment, making it essential to use low latency LLMs to power these chatbots.\n",
      "3. **Real-time Translation**: Imagine being in a foreign country and needing to communicate with a local. Real-time translation apps rely on LLMs to translate text or speech in real-time. Low latency is critical to ensure that the translation is accurate and delivered quickly, allowing for smooth communication.\n",
      "\n",
      "To illustrate the importance of latency, consider this analogy: Think of a LLM as a restaurant. When you order food, you expect it to arrive quickly, right? If the food takes too long to arrive, you might get frustrated and leave. Similarly, when interacting with an LLM, you expect a quick response. High latency is like a long wait for your food – it's frustrating and can lead to abandonment.\n",
      "\n",
      "In conclusion, low latency LLMs are essential for providing a seamless and responsive user experience. By minimizing latency, we can create more efficient and effective interactions between humans and machines. As computer science students, understanding the importance of latency will help you design and develop more user-friendly and responsive systems.\n"
     ]
    }
   ],
   "source": [
    "# Task + Context\n",
    "prompt = \"\"\"You are explaining to undergraduate computer science students who know basic systems concepts.\n",
    "Explain the importance of low latency LLMs and give 3 real-world examples where latency matters.\n",
    "Keep the explanation accessible and include one short analogy.\n",
    "\"\"\"\n",
    "\n",
    "print(get_completion(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23d828e",
   "metadata": {},
   "source": [
    "## 3 Examples (Zero-shot / Few-shot)\n",
    "\n",
    "**Zero-shot:** No examples. Rely on the task and context.\n",
    "\n",
    "**Few-shot:** Provide 1–3 short input→output examples so the model can infer the structure and style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e9bc784e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are three essential tips for writing effective code documentation:\n",
      "\n",
      "**1. Keep it concise and focused**:\n",
      "\n",
      "Good code documentation should be brief and to the point. Aim for a few sentences or a short paragraph at most. Avoid lengthy descriptions or unnecessary details that can confuse or overwhelm the reader. Focus on the essential information that a developer needs to understand the code, such as:\n",
      "\t* What the code does\n",
      "\t* How it works\n",
      "\t* Any assumptions or dependencies\n",
      "\t* Any potential issues or edge cases\n",
      "\n",
      "**2. Use clear and consistent language**:\n",
      "\n",
      "Use simple, clear language that is easy to understand. Avoid using technical jargon or overly complex terminology unless it's absolutely necessary. Use consistent formatting, headings, and syntax throughout your documentation to make it easy to scan and read. Consider using a standard documentation template or style guide to ensure consistency across your codebase.\n",
      "\n",
      "**3. Make it accessible and up-to-date**:\n",
      "\n",
      "Code documentation should be easily accessible to developers who need to understand and maintain the code. Make sure your documentation is:\n",
      "\t* Easily searchable and indexable\n",
      "\t* Linked to relevant code files or sections\n",
      "\t* Regularly updated to reflect changes to the code\n",
      "\t* Available in a format that is easy to read and understand (e.g., Markdown, HTML, PDF)\n",
      "\n",
      "By following these tips, you can create effective code documentation that helps developers understand and maintain your codebase, reducing errors, and improving overall productivity.\n"
     ]
    }
   ],
   "source": [
    "# Zero-shot example\n",
    "prompt = \"Provide 3 essential tips for writing effective code documentation.\"\n",
    "\n",
    "print(get_completion(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e5f5a96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are three essential tips for writing effective code documentation:\n",
      "\n",
      "**Tip 1: Be Consistent**\n",
      "\n",
      "Consistency is key to making your code documentation easy to read and understand. Establish a consistent format, tone, and style throughout your documentation. This includes using the same verb tenses, formatting, and terminology. Consistency will help readers quickly understand the structure and content of your documentation.\n",
      "\n",
      "**Tip 2: Focus on the Why, Not Just the What**\n",
      "\n",
      "While it's important to explain what your code does, it's equally important to explain why it does it. Providing context and motivation behind the code will help readers understand the purpose and intent behind the code. This can include explaining the problem being solved, the design decisions made, and the trade-offs considered.\n",
      "\n",
      "**Tip 3: Keep it Concise and Up-to-Date**\n",
      "\n",
      "Code documentation should be concise and to the point. Aim for a balance between providing enough information and not overwhelming the reader. Keep your documentation up-to-date by regularly reviewing and updating it as your code changes. This will help ensure that your documentation remains accurate and relevant.\n",
      "\n",
      "By following these tips, you can create effective code documentation that helps others understand and use your code efficiently.\n"
     ]
    }
   ],
   "source": [
    "# Few-shot example\n",
    "prompt = \"\"\"Here are some examples of good documentation tips:\n",
    "\n",
    "Input: \"How to write clear function names?\"\n",
    "Output: \"Use verbs, be specific, and indicate the return value (e.g., get_user_by_id, calculate_total_price).\"\n",
    "\n",
    "Input: \"How to document API endpoints?\"\n",
    "Output: \"Include HTTP method, parameters, response format, and authentication requirements.\"\n",
    "\n",
    "Now, provide 3 essential tips for writing effective code documentation.\"\"\"\n",
    "\n",
    "print(get_completion(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba216a7",
   "metadata": {},
   "source": [
    "## 4. Role\n",
    "\n",
    "Tell the model who it should *act as* (teacher, peer, senior engineer, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9c03b0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an experienced software engineer teaching undergrads, I'd be happy to explain the trade-offs between low latency and model size for Large Language Models (LLMs) in 4 bullet points:\n",
      "\n",
      "• **Latency vs. Model Size: The Bigger the Model, the Slower the Response**: Larger language models require more computational resources and memory to process, which can lead to slower response times. Conversely, smaller models can respond faster but may not be as accurate or capable. A good balance between model size and latency is crucial for real-world applications.\n",
      "\n",
      "• **Smaller Models are Faster, but Less Accurate**: Smaller models are often faster because they require fewer computations and less memory. However, they may not be as accurate as larger models, which can be trained on more data and have more complex architectures. This trade-off is particularly important for applications where accuracy is critical, such as natural language processing and machine translation.\n",
      "\n",
      "• **Larger Models are More Accurate, but Slower**: Larger models can be more accurate because they can capture more complex patterns and relationships in the data. However, they require more computational resources and memory, which can lead to slower response times. This trade-off is particularly important for applications where speed is critical, such as chatbots and virtual assistants.\n",
      "\n",
      "• **Model Pruning and Quantization can Help Bridge the Gap**: Model pruning and quantization are techniques that can help reduce the size of a model while maintaining its accuracy. Pruning involves removing unnecessary neurons or connections, while quantization involves reducing the precision of the model's weights and activations. These techniques can help reduce the latency of a model while maintaining its accuracy, making them particularly useful for real-world applications.\n",
      "\n",
      "I hope these bullet points help illustrate the trade-offs between low latency and model size for LLMs!\n"
     ]
    }
   ],
   "source": [
    "# Role example\n",
    "prompt = \"\"\"You are an experienced software engineer teaching undergrads.\n",
    "Explain the trade-offs between low latency and model size for LLMs in 4 bullet points.\"\"\"\n",
    "\n",
    "print(get_completion(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9f885c",
   "metadata": {},
   "source": [
    "## 5. Format\n",
    "\n",
    "Specify the structure of the output (bullets, JSON, table, code snippet, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d1c51c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a JSON object with the requested information:\n",
      "\n",
      "```\n",
      "{\n",
      "  \"summary\": \"Low latency enables faster response times, improving user experience and enabling real-time interactions with LLMs.\",\n",
      "  \"examples\": [\n",
      "    \"Faster response times allow for more efficient workflows and increased productivity.\",\n",
      "    \"Low latency enables real-time feedback and iteration, improving model accuracy and adaptability.\",\n",
      "    \"In applications like chatbots and virtual assistants, low latency is crucial for providing a seamless and responsive user experience.\"\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "The summary is under 80 characters, and the examples provide more detailed information on why low latency matters for LLMs.\n"
     ]
    }
   ],
   "source": [
    "# Format example (JSON)\n",
    "prompt = \"\"\"Return a JSON object with keys 'summary' (string) and 'examples' (list of strings)\n",
    "about why low latency matters for LLMs. Keep 'summary' under 80 chars.\"\"\"\n",
    "\n",
    "print(get_completion(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba2cba1",
   "metadata": {},
   "source": [
    "## 6. Tone\n",
    "\n",
    "The voice or attitude for the response (formal, friendly, concise, humorous, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb998e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're on the right track! Low latency LLMs are super important because they enable fast and efficient processing of natural language, allowing for more seamless and responsive interactions. This means you can get answers and insights quickly, without waiting around for slow responses!\n"
     ]
    }
   ],
   "source": [
    "# Tone example\n",
    "prompt = \"\"\"You are a friendly tutor who speaks in short, encouraging sentences.\n",
    "Explain why low latency LLMs are important in 2 sentences.\n",
    "\"\"\"\n",
    "\n",
    "print(get_completion(prompt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b064183b",
   "metadata": {},
   "source": [
    "# Chain-of-Thought (CoT) Techniques\n",
    "\n",
    "Chain-of-thought prompts instruct the model to reveal intermediate reasoning steps. Use them carefully:\n",
    "\n",
    "- **When to use:** debugging, teaching, transparent reasoning.\n",
    "- **When to avoid:** tasks requiring concise or private outputs, or when you don't want the model to hallucinate extra steps.\n",
    "\n",
    "**Controlled-CoT pattern (preferred):** Ask for a short step-by-step reasoning section followed by a final concise answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "55a62a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the 3 short reasoning steps:\n",
      "\n",
      "1. A smaller model with caching can quickly respond to user input, reducing latency and improving initial interactions.\n",
      "2. However, a larger model can provide more accurate and informative responses, enhancing the overall user experience.\n",
      "3. Caching can be effective for frequently accessed data, but may not be sufficient for complex or dynamic conversations.\n",
      "\n",
      "Answer: A larger model with caching is recommended, as it balances the need for accurate responses with the importance of fast initial interactions and efficient data retrieval.\n"
     ]
    }
   ],
   "source": [
    "# Cell: Controlled Chain-of-Thought example\n",
    "prompt = \"\"\"You are a clear explainer for undergrads. First give a numbered list of 3 short reasoning steps (each < 20 words), then provide a single-line final answer prefixed with 'Answer:'.\n",
    "Should a chat application prioritize a smaller model with caching or a larger model for better user experience? Give one clear recommendation and justify it.\n",
    "\"\"\"\n",
    "\n",
    "print(get_completion(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057f5265",
   "metadata": {},
   "source": [
    "**Note:** Explicit CoT can sometimes increase risk of hallucination and token use. If the model is unreliable with CoT, use few-shot examples of the expected reasoning format instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec18265",
   "metadata": {},
   "source": [
    "# Combined Example: Design a prompt step-by-step\n",
    "\n",
    "We'll build a high-quality prompt iteratively and then send it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7ef161f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello there! I'm excited to explain why low latency is crucial for Large Language Models (LLMs).\n",
      "\n",
      "Here are three key points:\n",
      "\n",
      "• **Response time matters**: When you interact with a language model, you expect a quick response. Low latency ensures that the model responds rapidly, making the experience feel more natural and engaging.\n",
      "• **Real-time feedback is essential**: LLMs are designed to learn from user interactions. Low latency enables the model to receive and process feedback in real-time, which is vital for improving its performance and accuracy.\n",
      "• **User experience suffers with high latency**: Imagine waiting for what feels like an eternity for a response. High latency can lead to frustration and a poor user experience, which is detrimental to the model's adoption and success.\n",
      "\n",
      "Let's consider a real-world example: **Google's Smart Compose feature**. When you start typing an email, Google's LLM predicts the next word or phrase and suggests it to you. If the latency is high, the suggestions might appear delayed or even after you've finished typing. This would disrupt the seamless experience that Google aims to provide.\n",
      "\n",
      "Here's an analogy to help you remember: **Low latency is like a quick conversation with a friend**. You want to respond to each other quickly, without long pauses, to maintain a natural and engaging conversation.\n",
      "\n",
      "Now, I'd like to suggest two follow-up study topics to help you dive deeper:\n",
      "\n",
      "1. **Explore the concept of latency in different networking scenarios**: Think about how latency affects the performance of web applications, online games, and other real-time systems.\n",
      "2. **Investigate the trade-offs between latency and other LLM performance metrics**: Consider how latency relates to other important factors like model accuracy, computational resources, and data storage.\n",
      "\n",
      "I hope this explanation and the follow-up suggestions help you better understand the importance of low latency for LLMs!\n"
     ]
    }
   ],
   "source": [
    "# Build combined prompt\n",
    "\n",
    "prompt = \"\"\"You are a patient lecturer for 2nd-year undergrad CS students. Keep things simple and avoid jargon.\n",
    "Your task is to explain why low latency is important for LLMs.\n",
    "Students know basic networking and web apps but not ML internals.\n",
    "Provide a 3-bullet explanation, then one real-world example, then a 1-sentence analogy.\n",
    "Act friendly and concise.\n",
    "Please also include 2 short follow-up study suggestions for students.\n",
    "\"\"\"\n",
    "\n",
    "print(get_completion(prompt))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
